{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d35c46f8d1a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpytorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch'"
     ]
    }
   ],
   "source": [
    "import pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import *\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.regularizers import *\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# set seed for random number generators to get repeateable results\n",
    "fixed_seed_num = 1234\n",
    "np.random.seed(fixed_seed_num)\n",
    "tf.set_random_seed(fixed_seed_num)\n",
    "\n",
    "def generator_model(x_shape,y_shape):\n",
    "    \n",
    "    # encoder\n",
    "    generator_input = Input(batch_shape=(None,x_shape,y_shape, 1), name='generator_input')\n",
    "    generator_input_normalized = BatchNormalization()(generator_input)\n",
    "    \n",
    "    conv1_32 = Conv2D(16,kernel_size=(3,3),strides=(1,1),padding='same',activation='elu',kernel_regularizer=l2(0.001))(generator_input)\n",
    "    conv1_32 = BatchNormalization()(conv1_32)\n",
    "    \n",
    "    conv2_64 = Conv2D(32,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv1_32)\n",
    "    conv2_64 = Conv2D(32,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv2_64)    \n",
    "    conv2_64 = MaxPooling2D(pool_size=(2,2),padding=\"same\")(conv2_64)\n",
    "    conv2_64 = BatchNormalization()(conv2_64)\n",
    "    \n",
    "    conv3_128 = Conv2D(64,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv2_64)\n",
    "    conv3_128 = Conv2D(64,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv3_128)\n",
    "    conv3_128 = MaxPooling2D(pool_size=(2,2),padding=\"same\")(conv3_128)\n",
    "    conv3_128 = BatchNormalization()(conv3_128)\n",
    "    \n",
    "    conv4_256 = Conv2D(128,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv3_128)\n",
    "    conv4_256 = Conv2D(128,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv4_256)\n",
    "    conv4_256 = MaxPooling2D(pool_size=(2,2),padding=\"same\")(conv4_256)\n",
    "    conv4_256 = BatchNormalization()(conv4_256)\n",
    "    \n",
    "    conv5_512 = Conv2D(256,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv4_256)\n",
    "    conv5_512 = Conv2D(256,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv5_512)\n",
    "    conv5_512 = MaxPooling2D(pool_size=(2,2),padding=\"same\")(conv5_512)\n",
    "    conv5_512 = BatchNormalization()(conv5_512)\n",
    "    \n",
    "    conv6_512 = Conv2D(512,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv5_512)\n",
    "    conv6_512 = Conv2D(512,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv5_512)\n",
    "    conv6_512 = MaxPooling2D(pool_size=(2,2),padding=\"same\")(conv6_512)\n",
    "    conv6_512 = BatchNormalization()(conv6_512)\n",
    "    \n",
    "    conv7_512 = Conv2D(512,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv6_512)\n",
    "    conv7_512 = BatchNormalization()(conv7_512)\n",
    "    \n",
    "    # decoder\n",
    "    conv8_512 = Conv2D(512,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(conv7_512)\n",
    "    conv8_512 = BatchNormalization(axis=1)(conv8_512)\n",
    "    \n",
    "    deconv9_512 = Conv2DTranspose(512,kernel_size=(3,3),padding='same',activation='elu',strides=(2,2),kernel_regularizer=l2(0.001))(conv8_512)\n",
    "    deconv9_512 = BatchNormalization()(deconv9_512)\n",
    "    deconv9_512 = Concatenate()([deconv9_512,conv5_512])\n",
    "    deconv9_512 = Conv2D(512,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(deconv9_512)\n",
    "    deconv9_512 = BatchNormalization()(deconv9_512)\n",
    "    \n",
    "    deconv10_256 = Conv2DTranspose(256,kernel_size=(3,3),padding='same',activation='elu',strides=(2,2),kernel_regularizer=l2(0.001))(deconv9_512)\n",
    "    deconv10_256 = BatchNormalization()(deconv10_256)\n",
    "    deconv10_256 = Concatenate()([deconv10_256,conv4_256])\n",
    "    deconv10_256 = Conv2D(256,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(deconv10_256)\n",
    "    deconv10_256 = BatchNormalization()(deconv10_256)\n",
    "    \n",
    "    deconv11_128 = Conv2DTranspose(128,kernel_size=(3,3),padding='same',activation='elu',strides=(2,2),kernel_regularizer=l2(0.001))(deconv10_256)\n",
    "    deconv11_128 = Concatenate()([deconv11_128,conv3_128])\n",
    "    deconv11_128 = Conv2D(128,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(deconv11_128)\n",
    "    \n",
    "    deconv12_64 = Conv2DTranspose(64,kernel_size=(3,3),padding='same',activation='elu',strides=(2,2),kernel_regularizer=l2(0.001))(deconv11_128)\n",
    "    deconv12_64 = Concatenate()([deconv12_64,conv2_64])\n",
    "    deconv12_64 = Conv2D(64,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(deconv12_64)\n",
    "    \n",
    "    deconv13_32 = Conv2DTranspose(32,kernel_size=(3,3),padding='same',activation='elu',strides=(2,2),kernel_regularizer=l2(0.001))(deconv12_64)\n",
    "    deconv13_32 = Concatenate()([deconv13_32,conv1_32])\n",
    "    deconv13_32 = Conv2D(32,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(deconv13_32)\n",
    "    \n",
    "    deconv14_16 = Conv2DTranspose(16,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(deconv13_32)\n",
    "    deconv14_16 = Conv2D(16,kernel_size=(3,3),padding='same',activation='elu',kernel_regularizer=l2(0.001))(deconv14_16)\n",
    "    \n",
    "    output = Conv2D(3,kernel_size=(1,1),padding='same',activation='relu')(deconv14_16)\n",
    "    \n",
    "    model = Model(inputs=generator_input,outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def discriminator_model(x_shape,y_shape):\n",
    "    \n",
    "    generator_input = Input(batch_shape=(None, x_shape, y_shape, 1), name='generator_output')\n",
    "    generator_output = Input(batch_shape=(None, x_shape, y_shape, 3), name='generator_input')\n",
    "    \n",
    "    input1 = BatchNormalization()(generator_input)\n",
    "    input2 = BatchNormalization()(generator_output)\n",
    "    \n",
    "    convi = Conv2D(32,kernel_size=(3,3),activation='elu',padding='same',kernel_regularizer=l2(0.001))(generator_input)\n",
    "    convi = BatchNormalization()(convi)\n",
    "    \n",
    "    convo = Conv2D(32,kernel_size=(3,3),activation='elu',padding='same',kernel_regularizer=l2(0.001))(generator_output)\n",
    "    convo = BatchNormalization()(convo)\n",
    "\n",
    "    \n",
    "    convi = Conv2D(64,kernel_size=(3,3),activation='elu',padding='same',kernel_regularizer=l2(0.001))(convi)\n",
    "    convi = BatchNormalization()(convi)\n",
    "    \n",
    "    convo = Conv2D(64,kernel_size=(3,3),activation='elu',padding='same',kernel_regularizer=l2(0.001))(convo)\n",
    "    convo = BatchNormalization()(convo)\n",
    "\n",
    "    \n",
    "    convi = Conv2D(64,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same',kernel_regularizer=l2(0.001))(convi)\n",
    "    convo = Conv2D(64,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same',kernel_regularizer=l2(0.001))(convo)\n",
    "    \n",
    "    conv = Concatenate()([convi,convo])\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    conv = Conv2D(128,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same',kernel_regularizer=l2(0.001))(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    conv = Conv2D(128,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same',kernel_regularizer=l2(0.001))(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    conv = Conv2D(128,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same',kernel_regularizer=l2(0.001))(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    conv = Conv2D(256,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same',kernel_regularizer=l2(0.001))(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    conv = Conv2D(256,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    conv = Flatten()(conv)\n",
    "    conv = Dropout(0.5)(conv)\n",
    "    \n",
    "    conv = Dense(100,activation='elu')(conv)\n",
    "    conv = Dropout(0.5)(conv)\n",
    "    \n",
    "    output = Dense(1,activation='sigmoid')(conv)\n",
    "    \n",
    "    model = Model(inputs=([generator_input,generator_output]),outputs=[output])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def cGAN_model(generator,discriminator):\n",
    "    \n",
    "    discriminator.trainable = False\n",
    "    model = Model(inputs=generator.inputs,outputs=[discriminator([generator.input,generator.output]), generator.output])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import losses\n",
    "def custom_loss(y_true,y_pred):\n",
    "    \n",
    "    cosine = losses.cosine_proximity(y_true,y_pred)\n",
    "    mle = losses.mean_absolute_error(y_true, y_pred)\n",
    "    l = (cosine)+mle\n",
    "    \n",
    "    return l\n",
    "\n",
    "def custom_loss_2(y_true,y_pred):\n",
    "    cosine = losses.cosine_proximity(y_true,y_pred)\n",
    "    mse = losses.mean_squared_error(y_true, y_pred)\n",
    "    mle = losses.mean_absolute_error(y_true, y_pred)\n",
    "    l = (cosine+1)*mse+mle\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(gen,disc,cGAN,gray,rgb,batch):\n",
    "    samples = len(rgb)\n",
    "    gen_image = gen.predict(gray, batch_size=16)   \n",
    "    gen_image_val = gen.predict(gray_val, batch_size=8)\n",
    "    inputs = np.concatenate([gray, gray])\n",
    "    outputs = np.concatenate([rgb, gen_image])\n",
    "    y = np.concatenate([np.ones((samples, 1)), np.zeros((samples, 1))])\n",
    "    disc.fit([inputs, outputs], y, epochs=1, batch_size=4)\n",
    "    disc.trainable = False\n",
    "    cGAN.fit(gray, [np.ones((samples, 1)), rgb], epochs=1, batch_size=batch, callbacks=[tensorboard],validation_data=[gray_val,[np.ones((val_samples,1)),rgb_val]])\n",
    "    disc.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 512, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([gray[0], gray[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = [512,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0930 01:29:53.137057  1100 deprecation_wrapper.py:119] From C:\\Users\\ryan_\\.conda\\envs\\forecasting_tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0930 01:29:53.475212  1100 deprecation_wrapper.py:119] From C:\\Users\\ryan_\\.conda\\envs\\forecasting_tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0930 01:29:53.607002  1100 deprecation_wrapper.py:119] From C:\\Users\\ryan_\\.conda\\envs\\forecasting_tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0930 01:29:53.607989  1100 deprecation_wrapper.py:119] From C:\\Users\\ryan_\\.conda\\envs\\forecasting_tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0930 01:29:53.608988  1100 deprecation_wrapper.py:119] From C:\\Users\\ryan_\\.conda\\envs\\forecasting_tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0930 01:29:54.094660  1100 deprecation_wrapper.py:119] From C:\\Users\\ryan_\\.conda\\envs\\forecasting_tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0930 01:29:54.474585  1100 deprecation_wrapper.py:119] From C:\\Users\\ryan_\\.conda\\envs\\forecasting_tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0930 01:29:57.515604  1100 deprecation.py:506] From C:\\Users\\ryan_\\.conda\\envs\\forecasting_tf\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0930 01:29:58.489313  1100 deprecation_wrapper.py:119] From C:\\Users\\ryan_\\.conda\\envs\\forecasting_tf\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0930 01:29:58.499292  1100 deprecation.py:323] From C:\\Users\\ryan_\\.conda\\envs\\forecasting_tf\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "gen = generator_model(temp[0], temp[1])\n",
    "\n",
    "disc = discriminator_model(temp[0], temp[1])\n",
    "\n",
    "cGAN = cGAN_model(gen, disc)\n",
    "\n",
    "disc.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08), metrics=['accuracy'])\n",
    "\n",
    "cGAN.compile(loss=['binary_crossentropy',custom_loss_2], loss_weights=[5, 100], optimizer=Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 1\n",
    "x_shape = 512\n",
    "y_shape = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = np.zeros((samples, x_shape, y_shape, 3))\n",
    "gray = np.zeros((samples, x_shape, y_shape, 1))\n",
    "y_train = np.zeros((samples,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(zoom_range=0.2,fill_mode='wrap',horizontal_flip=True,vertical_flip=True,\n",
    "                            rotation_range=15)\n",
    "datagen.fit(rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000 \n",
    "b=1\n",
    "for e in range(epochs):\n",
    "    print('Epoch', e)\n",
    "    for x_batch, y_batch in datagen.flow(rgb, y_train, batch_size=samples):\n",
    "        train(gen,disc,cGAN,gray,x_batch,gray_val,rgb_val,b)\n",
    "        batches += 1\n",
    "        if batches >= 1:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "    if e%5 == 0:\n",
    "        cGAN.save_weights(store2+str(e)+'.h5') \n",
    "    gen_image_val = gen.predict(gray_val, batch_size=8)\n",
    "    if e%1 == 0: \n",
    "        for j in range(val_samples):\n",
    "            if not os.path.exists(store2+str(j)+'/'):\n",
    "                os.mkdir(store2+str(j)+'/')\n",
    "            cv2.imwrite(store2+str(j)+'/'+str(e)+'.jpg', gen_image_val[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000 \n",
    "b=1\n",
    "for e in range(epochs):\n",
    "    print('Epoch', e)\n",
    "    batches = 0\n",
    "    for x_batch, y_batch in datagen.flow(rgb, y_train, batch_size=samples):\n",
    "        for i in range(len(x_batch)):\n",
    "            gray[i] = cv2.cvtColor(x_batch[i], cv2.COLOR_BGR2GRAY).reshape((x_shape,y_shape,1))\n",
    "        train(gen,disc,cGAN,gray,x_batch,gray_val,rgb_val,b)\n",
    "        batches += 1\n",
    "        if batches >= 1:\n",
    "            # we need to break the loop by hand because\n",
    "            # the generator loops indefinitely\n",
    "            break\n",
    "    if e%5 == 0:\n",
    "        cGAN.save_weights(store2+str(e)+'.h5') \n",
    "    gen_image_val = gen.predict(gray_val, batch_size=8)\n",
    "    if e%1 == 0: \n",
    "        for j in range(val_samples):\n",
    "            if not os.path.exists(store2+str(j)+'/'):\n",
    "                os.mkdir(store2+str(j)+'/')\n",
    "            cv2.imwrite(store2+str(j)+'/'+str(e)+'.jpg', gen_image_val[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image in enumerate(os.listdir(dataset)[:samples]):\n",
    "    I = cv2.imread(dataset+image)\n",
    "    I = cv2.resize(I, (x_shape, y_shape))\n",
    "#     J = cv2.cvtColor(I, cv2.COLOR_BGR2GRAY)\n",
    "#     J = J.reshape(J.shape[0], J.shape[1], 1)\n",
    "    rgb[i] = I;\n",
    "#     gray[i] = J\n",
    "\n",
    "for i, image in enumerate(os.listdir(val_data)[:val_samples]):\n",
    "    I = cv2.imread(val_data+image)\n",
    "    I = cv2.resize(I, (x_shape, y_shape))\n",
    "    J = cv2.cvtColor(I, cv2.COLOR_BGR2GRAY)\n",
    "    J = J.reshape(J.shape[0], J.shape[1], 1)\n",
    "    rgb_val[i] = I; gray_val[i] = J"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
