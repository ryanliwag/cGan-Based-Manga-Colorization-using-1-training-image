{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "  \n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def show_image(image):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loader\n",
    "transforms_orig = transforms.Compose([\n",
    "                transforms.RandomRotation(30),\n",
    "                transforms.Resize((16,16), Image.BICUBIC),\n",
    "                transforms.RandomHorizontalFlip()])\n",
    "\n",
    "transforms_bw = transforms.Compose([\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.ToTensor()])\n",
    "\n",
    "\n",
    "transforms_rgb = transforms.Compose([\n",
    "                transforms.ToTensor()])\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_orig=None, transforms_bw=None, transforms_rgb=None):\n",
    "        '''\n",
    "        Args:\n",
    "            csv_file (string)\n",
    "            transform (callable, optional)\n",
    "        '''\n",
    "        \n",
    "        files = glob.glob('%s/*.*' % root)\n",
    "        self.training_files = files * 64\n",
    "        self.transforms_orig = transforms_orig\n",
    "        self.transforms_rgb = transforms_rgb\n",
    "        self.transforms_bw = transforms_bw\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.training_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.training_files[idx])\n",
    "        \n",
    "        orig_image = self.transforms_orig(img)\n",
    "        \n",
    "        true_image = self.transforms_rgb(orig_image)\n",
    "        bw_image = self.transforms_bw(orig_image)\n",
    "        \n",
    "        return {\"true_image\": true_image, \"gen_image\": bw_image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = ImageDataset(root=\"training_images/\", transforms_orig=transforms_orig,\n",
    "                                                    transforms_rgb=transforms_rgb, \n",
    "                                                    transforms_bw=transforms_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "dataloader = DataLoader(\n",
    "    training_generator,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 16, 16])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAIAAACQkWg2AAACxUlEQVR4nAXBy28bRRwA4JnfzD5mH157ncTBTto6IQ/aqiniISpVAi5cKCdOcEUc+vf0wJkzSAgh0RMgIdoKqYlCFdFAmsQ4rePUXnttx7uzu7Pz4/uo5/o3Nm8BY5QQJMSyTD/wxqOh5/tpmqZSGpY465zKLM2yrFA5F7bwPI9SChQoQJ4ng/5ZHA2jCyAahV9ZWljmq+uqVKpURZFz3/V916eUAsB4/Lrf62hdAgBBJJQks8mr7vHiwooNtipVqRQPKlXXcQml8XjQP/8PURNKS60RkTOGiMnlZASs1Vqj1C5UAZZla0TTNIUtUGtEdC2zEQa3Nq8pVS6GAQCV6aWwbN+reI7HFTEOj58Ly5bzaVbk22tX7n/5GTDGDf7wtyd33r7+4NsfLqI4TWf1cEFKzhvL693T3ZGcOyYLA//rLz6998ndVOaFUu/vbLuO2D148d3D38NwwRaCAuXN1TfH0WkcD9+5uXXv7k2vEj76Y6+52vA8R0pJCb6xFK5dba80ryBqSjS/tvHW4OJIqaI3mNUqbn1lbT6YCtdKZP5s9yWvee/tbC63tjr9tChkiQYE1XqjuVENWwl6v+732otseyvgFAPfvfPR7Xd31gFhnoFhMNO0VKng5ODpJOrP5xMC9M/nvf1/zpISNJhIqGuoeBT/3WeJIowzbhqpTPmLo/3xqB8NupSZ3AoePdlrNeqFVYUskReDp8epLCmlSChlDGaXUz6ZDJUqZSZ1mQitDJWeHx4I3weAvzqjkwhcYQIwAOCcT2YxL1SpVbqxZPamxVeff/zh9atJIm1bVMJqfVZ88/1PShNhC9e2XMc5/PeID6NOw8Way85m+GoQ1Vsf1FDbjpNn+e3NVqthn56cvB6VcaI5N4bROe92O7HgL21jrowff3l8o93c2W7XkADiz4/3nh112lVWd3A0nSZplmfz/wFbXm3orY36dgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=16x16 at 0x2A8BECF7BA8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAAAAAA6mKC9AAABEElEQVR4nAXBy0rDQBQA0Jmba5JpkqYYta2pIFQhgmjBreDSP3Aj/pprf8CN4ErElQtFFFsfWLDUGJLUPjJJJjOeQ61AI8Sox07G9S+eC+rtUoB8EYOyOxNRlWjbFJJvCYpMh6umqNC1SDJSVCpNzaBjCjCkzpQyvUB4kJmOhWpgzovumbZ0fXAeLZY5tj9SvXF6zEWv9njpMcCNKN07cm87NifNTV8R3BqL0PWTWvbg9PxxIaGx7sn75jbUD/dJjrqA53gGTy8lVXoyKlHn2I9DzbhrYfEzEJRqU5wILiv5ZsN7yDTAPxSinZ70FmYjuagYq/Xx17HTsKVYEax9JnOMcMhYedXdUeTmdcWaZPk//gN9Ph4OpgAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=16x16 at 0x2A8BECF7E80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 16, 16])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAIAAACQkWg2AAACyElEQVR4nE2TzU8TQQDFZ2Znd3a33dLSCtgAUmGpSgUKMWDwAP8A0URPYLx48OgfYuLdC/GgJ07oQROCUeSrAiIitKDQUmj5bAvbj+3s7owH1PjOL+/3Du9B8J8aGoJ6W9hXU2OWKWOcYEkQEEK4YpY+zE5SSgEA8MKqKqquX6sLBLLpJJEkiAQRk1pfHZEVQmRBwCtrS2vxVQAAxAIOhXRdb9/f3qRmmf+Bcc4BQkK4vdN/6bJhnB+ensaWZyqVIuy6Ea31BQQM9pJbEAIAAOOcc84ZRwh6PL5L9U37B/tYUgXRHYu9x16vXxKl46P031SoKbJLIfX+2vhOOpc/KRRLIpEBrFzvHMweJLEqK7ZFTbOEEBQxtm3nychwnd/bfUNfjf968XoiGAzOr+0ghF2a+mD0KVYVFShKNoMs23p4d1AmUk8kHAmHqGV36C2PR+7JkG2kchZDnIM2vQu7VBe6qM54byQ8NNDrMGZW6dZ26iBVvXYrmvi+iIlHkbXj7N7m6gKWJUmWldar4ZVvX7aS+/09HV9XNjw+r97a0tgMvSpcFxSGlXL5/DDzs2KcYiLLhJBIR9TjdtuOtZhIY6G+rsFfNSsSQkaVnBUrxfO8UchatAw4xPmzXEtjs0xIT7QvEnL5JNZ2M4gcSm1SBIpjM42XaDlnFA0F2yIvwubGK6P3RzXVJREiiaJHFZrq3e1NGoXI4YJ1mskkU8nDs+nE7o9EojUg4t291Prm+p2+AQABg+C8Ctb3yjuHxZC75Ndkm1JVdQ3dblECvpnVxPZxBQMAPs19jN7s1jQPZwwJqFotxZbjhZDv0XC/w4HjWJDx7NHJyfHRtmEgAEAun/u8MC1iwXas+NbmxLvJ+aUlr0fknFHTNE366u3U87HxQqHAHAdfbG02NtfeGt5NZ1KZI4fm/Yrz8s0U4natpj4bG59eWvt3gd9AHVSI0WlNvAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=16x16 at 0x2A8C93D0B70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAAAAAA6mKC9AAABFElEQVR4nAXBy0oCURgA4P/888/lzKQMYYxFCJGKXWyiVkGP0CJo0yJfqq3LHiBa9QBRELkoFY3CkspLETk5pGdGPafvYwAA6aw7lrqG4ioGBsBzCx8G0rxlatUmkLaSa79ADGpQsMMUH+PaohCMgZKy03/+2wJyjU9QaPNU62doMr9HfCKQpqXURvNsqYpzJ8Q5mx6ZxcIkX9LaM5Unh4Es7suo1fXrptmvk2Gt3r/uPrjZjIM46oZkmZuJWYM8gSIMgwgwMKyddb7t6BDjKFSKLR87hmGnM0r+vvVqjx6Dgz2mI+puMo68xqlFcO0npCYq2UM1VV/fQ4TBDc2eLisJJaKLciAJ4Db33onsc0iW7wDgH+uGbhuR9lPxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=16x16 at 0x2A8C93D09B0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 16, 16])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAIAAACQkWg2AAACzUlEQVR4nAXBTY/bRBgA4HnH74zHEydxNs3uJum6mwJtFxBUBcSXOHDjgIQop3LhQNsTVyT+ACfOHJEQEkjcQJyqikoV6gGBEGpBqXYbdjebj2Y3seM4TmyPx+Z5gBBCCGGMO06tVqtJYW0329FiLoRkTITBEoAQQig1RpPBfq+LbrtTkiXLtOqNTWTQ7z0e9B5nKuWmKJUdKR0pSnmeEyB1pw4A6LZcAxlnLFfpaDTQmQIgADRNkiQeL7nfbHXK5Q3DMCiAMAVKKREx1/npZBSvIwAoCGitAQAITdPYm443G00DGaJRrThYkjbnZhQFSRwxNAAgz4u95y7MvGCdpMFyFSfrOF4rHS1XkbDKaMuSZVmz6VBlym02333z6sHR6NaN92deYAn+5dffHw6edvcfFdRgTDRbz6KUUgghTKF1/vYrL372yUeL5cq2JWeMc/QX4Q8/3/NWauItLGm7uy9QIYQlxDMXL3MuNmoVkyPnhlJqMBzfu/t72dn6/Ob1zvk2Y1alWu9ceh5Nk5kM7dLmpx/fcNvy/h8H4WT+0qsXq051y7WdzUr34Z/Hk9BxGo3t3Z3dS5iqtFouM8Ttczvtut5ou2LvssUSFcedHQsN/U13CuKcRUlR0Em/h2ezs9Z2Cw3qR+qfIX2jumy1WaygQDPUfNwf/r0/jEIvXs3jpW8UOY6eDq+9fI0AAIVQ0QfdQGldqto5r7A4GvQO+ifHYejrLCm01jrD4XhYFDkiAyAU8mWc3f9r+nrH4gyTKOIqubrr/PbwlEOKFAy0cHI2mXlTQsCfB7N5MPX8KxcaH7zznk6zkJBaXnxx88OTr76F9VSy1aF3hItw8cudu4hiHa84VZrkt6+/ZjJUFBrnm06SPDroj06nJmQ1vvb9GIuiOD55YlDDZPDWlcbJovjxzoM9d6tql86Cxb9Pjr776df/Do+01lmmioL8Dy6xVi18qoMKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=16x16 at 0x2A8C93D0CC0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAAAAAA6mKC9AAABEElEQVR4nAXBv07CQBwA4Pv1fnc9TgpUxVj+JBCiiXFhcXLxIdyMg5M+iE/gg+hmXHwB4yYLRK1aBTS0qKW25Xqt3weEsJotnFDwX0Lo5BFbUtTx2VWmJUs52IAtZNk4A0gT3rQomChRT2MgGshytkGxgpJHCUK+HaRhEutI4kppljX23aNAXHhDYA2UwtR7J4sy4z9XkS+7hhA9bnOmPm5rp21W3UKTl4+bd36/2l57mNqbHVxabL3u7LC0RQemQTz0Hbrwqrai6mUY/kUEJ30wloOsgsp9CzOtjXFBAeJ77/0p7+VAEb988j0POgcKVs/OE9PH8AZjWhwyo5GOPg0VYPFK2e78umvNR5euVuQf1wlvKwygEugAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=16x16 at 0x2A8C93D0B70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "\n",
    "for idx, image in enumerate(dataloader):\n",
    "    res = to_pil(image[\"true_image\"][0])\n",
    "    bw = to_pil(image[\"gen_image\"][0])\n",
    "    print(image[\"true_image\"].shape)\n",
    "    display(res)\n",
    "    display(bw)\n",
    "    if idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \n",
    "    def unet_conv(self, ch_in, ch_out, is_leaky):\n",
    "        \"\"\"\n",
    "        Construct a convolutional unit comprising of two conv layers\n",
    "        followed by a batch normalisation layer and ReLU/Leaky ReLU.\n",
    "        \"\"\"\n",
    "        if is_leaky:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(ch_in, ch_out, 3, padding=1),\n",
    "                nn.BatchNorm2d(ch_out),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Conv2d(ch_out, ch_out, 3, padding=1),\n",
    "                nn.BatchNorm2d(ch_out),\n",
    "                nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(ch_in, ch_out, 3, padding=1),\n",
    "                nn.BatchNorm2d(ch_out),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(ch_out, ch_out, 3, padding=1),\n",
    "                nn.BatchNorm2d(ch_out),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "    def up(self, ch_in, ch_out):\n",
    "        \"\"\"\n",
    "        Applies a 2D bilinear upsampling to the input image which scales\n",
    "        the image 2x times, followed by a convolution with a 1x1 kernel. \n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(ch_in, ch_out, 3, 2, 1, 1), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def __init__(self, is_leaky):\n",
    "        \"\"\"\n",
    "        In the constructer, all the convolutional, upsampling and max pooling \n",
    "        units are instantiated and assigned as member variables. \n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # First encoding layer\n",
    "        self.conv1 = self.unet_conv(1, 64, is_leaky)\n",
    "        # Second encoding layer\n",
    "        self.conv2 = self.unet_conv(64, 128, is_leaky)\n",
    "        # Third encoding layer\n",
    "        self.conv3 = self.unet_conv(128, 256, is_leaky)\n",
    "        # Fourth encoding layer\n",
    "        self.conv4 = self.unet_conv(256, 512, is_leaky)\n",
    "        # Fifth encoding layer\n",
    "        self.conv5 = self.unet_conv(512, 1024, is_leaky)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # First Upsampling layer\n",
    "        self.up1 = self.up(1024, 512)\n",
    "        # Second Upsampling layer\n",
    "        self.up2 = self.up(512, 256)\n",
    "        # Third Upsampling layer\n",
    "        self.up3 = self.up(256, 128)\n",
    "        # Fourth Upsampling layer\n",
    "        self.up4 = self.up(128, 64)\n",
    "        \n",
    "        # First decoding layer\n",
    "        self.conv6 = self.unet_conv(1024, 512, False)\n",
    "        # Second decoding layer\n",
    "        self.conv7 = self.unet_conv(512, 256, False)\n",
    "        # Third decoding layer\n",
    "        self.conv8 = self.unet_conv(256, 128, False)\n",
    "        # Fourth decoding layer\n",
    "        self.conv9 = self.unet_conv(128, 64, False)\n",
    "        \n",
    "        # Last layer\n",
    "        self.conv10 = nn.Conv2d(64, 3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        An input tensor of a black and white image is accepted and\n",
    "        passed through the U-Net model. A colored image in CieLAB color\n",
    "        space is returned as the result. \n",
    "        \"\"\"\n",
    "        # Encoding path\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(self.pool(x1))\n",
    "        x3 = self.conv3(self.pool(x2))\n",
    "        x4 = self.conv4(self.pool(x3))\n",
    "        x5 = self.conv5(self.pool(x4))\n",
    "        \n",
    "        # Decoding path\n",
    "        x = self.conv6(torch.cat((x4, self.up1(x5)), 1))\n",
    "        x = self.conv7(torch.cat((x3, self.up2(x)), 1))\n",
    "        x = self.conv8(torch.cat((x2, self.up3(x)), 1))\n",
    "        x = self.conv9(torch.cat((x1, self.up4(x)), 1))\n",
    "        x = self.conv10(x)\n",
    "        m = nn.Tanh()\n",
    "        x = m(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class DNet(nn.Module):\n",
    "    \n",
    "    def unet_conv(self, ch_in, ch_out, generator_output=True):\n",
    "        \"\"\"\n",
    "        Construct a convolutional unit comprising of two conv layers\n",
    "        followed by a batch normalisation layer and Leaky ReLU.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, 3, padding=1),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(ch_out, ch_out, 3, padding=1),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DNet, self).__init__()\n",
    "        \"\"\"\n",
    "        In the constructer, all the convolutional and max pooling units \n",
    "        are instantiated and assigned as member variables. \n",
    "        \"\"\"\n",
    "        # First layer\n",
    "        self.conv1_x = self.unet_conv(3, 64)\n",
    "        \n",
    "        self.conv1_y = self.unet_conv(1, 64)\n",
    "        \n",
    "        # Second layer\n",
    "        self.conv2 = self.unet_conv(64, 128)\n",
    "        # Third layer\n",
    "        self.conv3 = self.unet_conv(128, 256)\n",
    "        # Fourth layer\n",
    "        self.conv4 = self.unet_conv(256, 512)\n",
    "        # Fifth layer\n",
    "        self.conv5 = self.unet_conv(512, 1024)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Last layer\n",
    "        self.conv6 = nn.Linear(4*1024, 1)\n",
    "\n",
    "    def forward(self, x_rgb, x_bw):\n",
    "        \"\"\" \n",
    "        An input tensor of a colored image from either the generator or source\n",
    "        is accepted and passed through the model. The probability of the image\n",
    "        belonging to the source domain is returned as the result. \n",
    "        \"\"\"\n",
    "        x1_rgb = self.conv1_x(x_rgb)\n",
    "        x2_rgb = self.conv2(self.pool(x1_rgb))\n",
    "        \n",
    "        x1_bw = self.conv1_y(x_bw)\n",
    "        x2_bw = self.conv2(self.pool(x1_bw))     \n",
    "        \n",
    "        x2 = torch.cat((x2_rgb, x2_bw), 1)\n",
    "        x4 = self.conv4(self.pool(x2))\n",
    "        x5 = self.conv5(self.pool(x4))\n",
    "\n",
    "        x6 = x5.view(batch_size, -1)\n",
    "        m = nn.Sigmoid()\n",
    "        x = m(self.conv6(x6))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch import cat\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "##############################################################\n",
    "# Initialise the generator and discriminator with the UNet and\n",
    "# DNet architectures respectively.\n",
    "generator = UNet(True)\n",
    "discriminator = DNet()\n",
    "\n",
    "##################################################################\n",
    "# Utilize GPU for performing all the calculations performed in the\n",
    "# forward and backward passes. Thus allocate all the generator and\n",
    "# discriminator variables on the default GPU device.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "###################################################################\n",
    "# Create ADAM optimizer for the generator as well the discriminator.\n",
    "# Create loss criterion for calculating the L1 and adversarial loss.\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), betas=(0.5, 0.999), lr=0.0002)\n",
    "g_optimizer = optim.Adam(generator.parameters(), betas=(0.5, 0.999), lr=0.0002)\n",
    "\n",
    "d_criterion = nn.BCELoss()\n",
    "g_criterion_1 = nn.BCELoss()\n",
    "g_criterion_2 = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(PATH):\n",
    "    g_lambda = 100\n",
    "    smooth = 0.1\n",
    "    for epoch in range(100):  \n",
    "        # the generator and discriminator losses are summed for the entire epoch.\n",
    "        d_running_loss = 0.0\n",
    "        g_running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader):\n",
    "            images = data\n",
    "            true_images = images[\"true_image\"].to(device)\n",
    "            bw_images = images[\"gen_image\"].to(device)\n",
    "\n",
    "#             print(true_images.shape, bw_images.shape)\n",
    "            fake_images = generator(bw_images)\n",
    "\n",
    "            d_optimizer.zero_grad()\n",
    "            d_loss = 0\n",
    "            \n",
    "            logits = discriminator(true_images, bw_images)\n",
    "            \n",
    "            d_real_loss = d_criterion(logits, ((1 - smooth) * torch.ones(batch_size)).to(device))\n",
    "            \n",
    "            logits = discriminator(fake_images, bw_images)\n",
    "            d_fake_loss = d_criterion(logits, (torch.zeros(batch_size)).to(device))\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            d_optimizer.step()\n",
    "\n",
    "            # Train the generator. The loss would be the sum of the adversarial loss\n",
    "            # due to the GAN and L1 distance loss between the fake and target images. \n",
    "            g_optimizer.zero_grad()\n",
    "            g_loss = 0\n",
    "            fake_logits = discriminator(fake_images, bw_images)\n",
    "            g_fake_loss = g_criterion_1(fake_logits, (torch.ones(batch_size)).to(device))\n",
    "\n",
    "            g_image_distance_loss = g_lambda * g_criterion_2(fake_images, true_images)\n",
    "\n",
    "            g_loss = g_fake_loss + g_image_distance_loss\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            d_running_loss += d_loss\n",
    "            g_running_loss += g_loss\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            print('[%d, %5d] d_loss: %.5f g_loss: %.5f' %\n",
    "                  (epoch + 1, i + 1, d_running_loss / 10, g_running_loss / 10))\n",
    "            d_running_loss = 0.0\n",
    "            g_running_loss = 0.0\n",
    "            \n",
    "            #save generator and descr\n",
    "            torch.save(generator.state_dict(), os.path.join(PATH, \"generator_epoch_{}\".format(epoch)))\n",
    "            torch.save(discriminator.state_dict(), os.path.join(PATH, \"descriminator_epoch_{}\".format(epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    32] d_loss: 4.20789 g_loss: 44.57426\n",
      "[11,    32] d_loss: 4.36780 g_loss: 6.36146\n",
      "[21,    32] d_loss: 4.38168 g_loss: 5.26617\n",
      "[31,    32] d_loss: 4.39745 g_loss: 4.74114\n",
      "[41,    32] d_loss: 4.40064 g_loss: 4.44628\n",
      "[51,    32] d_loss: 4.40195 g_loss: 4.36259\n",
      "[61,    32] d_loss: 4.40126 g_loss: 4.36022\n"
     ]
    }
   ],
   "source": [
    "train(PATH=\"model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tTrain the dataset for several epochs. \n",
    "\t\"\"\"\n",
    "\tg_lambda = 100\n",
    "\tsmooth = 0.1\n",
    "\n",
    "\t# loop over the dataset multiple times.\n",
    "\tfor epoch in range(200):  \n",
    "\t\t# the generator and discriminator losses are summed for the entire epoch.\n",
    "\t    d_running_loss = 0.0\n",
    "\t    g_running_loss = 0.0\n",
    "\t    for i, data in enumerate(cielab_loader):\n",
    "\t        lab_images = data\n",
    "\t        # split the lab color space images into luminescence and chrominance channels.\n",
    "\t        l_images = lab_images[:, 0, :, :]\n",
    "\t        c_images = lab_images[:, 1:, :, :]\n",
    "\t        # shift the source and target images into the range [-0.5, 0.5].\n",
    "\t        mean = torch.Tensor([0.5])\n",
    "\t        l_images = l_images - mean.expand_as(l_images)\n",
    "\t        l_images = 2 * l_images\n",
    "\t        \n",
    "\t        c_images = c_images - mean.expand_as(c_images)\n",
    "\t        c_images = 2 * c_images\n",
    "\t        # allocate the images on the default gpu device.\n",
    "\t        batch_size = l_images.shape[0]\n",
    "\t        l_images = Variable(l_images.cuda())\n",
    "\t        c_images = Variable(c_images.cuda())\n",
    "\t        # fake images are generated by passing them through the generator.\n",
    "\t        fake_images = generator(l_images)\n",
    "\t        \n",
    "\t        # Train the discriminator. The loss would be the sum of the losses over\n",
    "\t        # the source and fake images, with greyscale images as the condition.\n",
    "\t        d_optimizer.zero_grad()\n",
    "\t        d_loss = 0\n",
    "\t        logits = discriminator(cat([l_images, c_images], 1))\n",
    "\t        d_real_loss = d_criterion(logits, ((1 - smooth) * torch.ones(batch_size)).cuda())\n",
    "\t        \n",
    "\t        logits = discriminator(cat([l_images, fake_images], 1))\n",
    "\t        d_fake_loss = d_criterion(logits, (torch.zeros(batch_size)).cuda())\n",
    "\n",
    "\t        d_loss = d_real_loss + d_fake_loss\n",
    "\t        d_loss.backward()\n",
    "\t        d_optimizer.step()\n",
    "\n",
    "\t        # Train the generator. The loss would be the sum of the adversarial loss\n",
    "\t        # due to the GAN and L1 distance loss between the fake and target images. \n",
    "\t        g_optimizer.zero_grad()\n",
    "\t        g_loss = 0\n",
    "\t        fake_logits = discriminator(cat([l_images, fake_images], 1))\n",
    "\t        g_fake_loss = g_criterion_1(fake_logits, (torch.ones(batch_size)).cuda())\n",
    "\t        \n",
    "\t        g_image_distance_loss = g_lambda * g_criterion_2(fake_images, c_images)\n",
    "\n",
    "\t        g_loss = g_fake_loss + g_image_distance_loss\n",
    "\t        g_loss.backward()\n",
    "\t        g_optimizer.step()\n",
    "\t        \n",
    "\t        # print statistics on pre-defined intervals.\n",
    "\t        d_running_loss += d_loss\n",
    "\t        g_running_loss += g_loss\n",
    "\t        if i % 10 == 0:\n",
    "\t            print('[%d, %5d] d_loss: %.5f g_loss: %.5f' %\n",
    "\t                  (epoch + 1, i + 1, d_running_loss / 10, g_running_loss / 10))\n",
    "\t            d_running_loss = 0.0\n",
    "\t            g_running_loss = 0.0\n",
    "\t    \n",
    "\t    # save the generator and discriminator state after each epoch.\n",
    "\t    torch.save(generator.state_dict(), 'home/cifar10_train_generator')\n",
    "\t    torch.save(discriminator.state_dict(), 'home/cifar10_train_discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    generator_input = Input(batch_shape=(None, x_shape, y_shape, 1), name='generator_output')\n",
    "    generator_output = Input(batch_shape=(None, x_shape, y_shape, 3), name='generator_input')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DescNN(nn.Module):\n",
    "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
    "        super(self).__init__()\n",
    "        \n",
    "        model = [nn.Conv2d(in_size, out_size, kernel_size=(3,3))]\n",
    "        if normalize:\n",
    "            model.append(nn.BatchNorm2d(out_size))\n",
    "            \n",
    "        if dropout:\n",
    "            model.append(nn.Dropout(dropout))\n",
    "            \n",
    "        self.model = nn.Sequential(*model)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    generator_input = Input(batch_shape=(None, x_shape, y_shape, 1), name='generator_output')\n",
    "    generator_output = Input(batch_shape=(None, x_shape, y_shape, 3), name='generator_input')\n",
    "    \n",
    "    input1 = BatchNormalization()(generator_input)\n",
    "    input2 = BatchNormalization()(generator_output)\n",
    "    \n",
    "    convi = Conv2D(32,kernel_size=(3,3),activation='elu',padding='same',kernel_regularizer=l2(0.001))(generator_input)\n",
    "    convi = BatchNormalization()(convi)\n",
    "    \n",
    "    convo = Conv2D(32,kernel_size=(3,3),activation='elu',padding='same',kernel_regularizer=l2(0.001))(generator_output)\n",
    "    convo = BatchNormalization()(convo)\n",
    "\n",
    "       \n",
    "    convi = Conv2D(64,kernel_size=(3,3),activation='elu',padding='same',kernel_regularizer=l2(0.001))(convi)\n",
    "    convi = BatchNormalization()(convi)\n",
    "    \n",
    "    convo = Conv2D(64,kernel_size=(3,3),activation='elu',padding='same',kernel_regularizer=l2(0.001))(convo)\n",
    "    convo = BatchNormalization()(convo)\n",
    "\n",
    "    \n",
    "    convi = Conv2D(64,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same',kernel_regularizer=l2(0.001))(convi)\n",
    "    convo = Conv2D(64,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same',kernel_regularizer=l2(0.001))(convo)\n",
    "    \n",
    "    conv = Concatenate()([convi,convo])\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    conv = Conv2D(128,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same',kernel_regularizer=l2(0.001))(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    conv = Conv2D(128,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same',kernel_regularizer=l2(0.001))(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    conv = Conv2D(128,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same',kernel_regularizer=l2(0.001))(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    conv = Conv2D(256,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same',kernel_regularizer=l2(0.001))(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    conv = Conv2D(256,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    conv = Flatten()(conv)\n",
    "    conv = Dropout(0.5)(conv)\n",
    "    \n",
    "    conv = Dense(100,activation='elu')(conv)\n",
    "    conv = Dropout(0.5)(conv)\n",
    "    \n",
    "    output = Dense(1,activation='sigmoid')(conv)\n",
    "    \n",
    "    model = Model(inputs=([generator_input,generator_output]),outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        channels, height, width = input_shape\n",
    "        # Calculate output of image discriminator (PatchGAN)\n",
    "        patch_h, patch_w = int(height / 2 ** 3), int(width / 2 ** 3)\n",
    "        self.output_shape = (1, patch_h, patch_w)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, stride, normalize):\n",
    "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        layers = []\n",
    "        in_filters = channels\n",
    "        for out_filters, stride, normalize in [(64, 2, False), (128, 2, True), (256, 2, True), (512, 1, True)]:\n",
    "            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n",
    "            in_filters = out_filters\n",
    "\n",
    "        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
    "        super(UNetDown, self).__init__()\n",
    "        model = [nn.Conv2d(in_size, out_size, 4, stride=2, padding=1, bias=False)]\n",
    "        if normalize:\n",
    "            model.append(nn.BatchNorm2d(out_size, 0.8))\n",
    "        model.append(nn.LeakyReLU(0.2))\n",
    "        if dropout:\n",
    "            model.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_size, out_size, dropout=0.0):\n",
    "        super(UNetUp, self).__init__()\n",
    "        model = [\n",
    "            nn.ConvTranspose2d(in_size, out_size, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_size, 0.8),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        if dropout:\n",
    "            model.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        out = torch.cat((x, skip_input), 1)\n",
    "        return out\n",
    "\n",
    "class Descriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(self).__init__()\n",
    "        channels, _, _ =input_shape\n",
    "        \n",
    "        self.conv1 = DescNN(32, 64)\n",
    "        self.conv2 = DescNN(64, 128)\n",
    "        \n",
    "        self.conv3 = DescNN(128, 128)\n",
    "        self.conv4 = \n",
    "        \n",
    "            \n",
    "    convi = Conv2D(32,kernel_size=(3,3),activation='elu',padding='same',kernel_regularizer=l2(0.001))(generator_input)\n",
    "    convi = BatchNormalization()(convi)\n",
    "    \n",
    "    convo = Conv2D(32,kernel_size=(3,3),activation='elu',padding='same',kernel_regularizer=l2(0.001))(generator_output)\n",
    "    convo = BatchNormalization()(convo)\n",
    "\n",
    "       \n",
    "    convi = Conv2D(64,kernel_size=(3,3),activation='elu',padding='same',kernel_regularizer=l2(0.001))(convi)\n",
    "    convi = BatchNormalization()(convi)\n",
    "    \n",
    "    convo = Conv2D(64,kernel_size=(3,3),activation='elu',padding='same',kernel_regularizer=l2(0.001))(convo)\n",
    "    convo = BatchNormalization()(convo)\n",
    "\n",
    "    \n",
    "    convi = Conv2D(64,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same',kernel_regularizer=l2(0.001))(convi)\n",
    "    convo = Conv2D(64,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same',kernel_regularizer=l2(0.001))(convo)\n",
    "    \n",
    "    conv = Concatenate()([convi,convo])\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    conv = Conv2D(128,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same',kernel_regularizer=l2(0.001))(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    conv = Conv2D(128,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same',kernel_regularizer=l2(0.001))(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    conv = Conv2D(128,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same',kernel_regularizer=l2(0.001))(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    conv = Conv2D(256,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same',kernel_regularizer=l2(0.001))(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    conv = Conv2D(256,kernel_size=(3,3),strides=(2,2),activation='elu',padding='same')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    \n",
    "    conv = Flatten()(conv)\n",
    "    conv = Dropout(0.5)(conv)\n",
    "    \n",
    "    conv = Dense(100,activation='elu')(conv)\n",
    "    conv = Dropout(0.5)(conv)\n",
    "    \n",
    "    output = Dense(1,activation='sigmoid')(conv)\n",
    "    \n",
    "        \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        channels, _, _ = input_shape\n",
    "        self.down1 = UNetDown(channels, 64, normalize=False)\n",
    "        self.down2 = UNetDown(64, 128)\n",
    "        self.down3 = UNetDown(128 + channels, 256, dropout=0.5)\n",
    "        self.down4 = UNetDown(256, 512, dropout=0.5)\n",
    "        self.down5 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down6 = UNetDown(512, 512, dropout=0.5)\n",
    "\n",
    "        self.up1 = UNetUp(512, 512, dropout=0.5)\n",
    "        self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up3 = UNetUp(1024, 256, dropout=0.5)\n",
    "        self.up4 = UNetUp(512, 128)\n",
    "        self.up5 = UNetUp(256 + channels, 64)\n",
    "\n",
    "        final = [nn.Upsample(scale_factor=2), nn.Conv2d(128, channels, 3, 1, 1), nn.Tanh()]\n",
    "        self.final = nn.Sequential(*final)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # U-Net generator with skip connections from encoder to decoder\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "#         d2 = torch.cat((d2, x_lr), 1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        u1 = self.up1(d6, d5)\n",
    "        u2 = self.up2(u1, d4)\n",
    "        u3 = self.up3(u2, d3)\n",
    "        u4 = self.up4(u3, d2)\n",
    "        u5 = self.up5(u4, d1)\n",
    "\n",
    "        return self.final(u5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
